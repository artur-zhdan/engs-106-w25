{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXCRtFOAtZDE"
   },
   "source": [
    "\n",
    "```\n",
    "ROMEO:\n",
    "And from the embracement be spokes to stand,\n",
    "As we shall breathest to the market-fairly maid\n",
    "So month in my father, I may see thee not my side\n",
    "And love the prisoner like a cradist of my daughter.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93HWBje3tZDI"
   },
   "source": [
    "The above text is not a lost work of Shakespeare but a fully generated text by a GPT2-like model I trained on my laptop in less than 20 minutes. Today, in this tutorial, we will follow an implementation of the \"Attention Is All You Need\" paper, so that you can generate your own Shakespeare at home."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 86,
     "status": "ok",
     "timestamp": 1739738951131,
     "user": {
      "displayName": "Moin Mattar",
      "userId": "09338961536956502865"
     },
     "user_tz": 300
    },
    "id": "JxX8m0pStZDJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from dataset import getData, getVocabSize\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "from utils import train, inference\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwO0g2_jtZDK"
   },
   "source": [
    "Below, we define all the parameters used for training and to describe the\n",
    "\n",
    "---\n",
    "\n",
    "model. Please feel free to modify any parameters described except for certain\n",
    "\n",
    "---\n",
    "\n",
    "marked with ``DO NOT MODIFY``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 112,
     "status": "ok",
     "timestamp": 1739738951368,
     "user": {
      "displayName": "Moin Mattar",
      "userId": "09338961536956502865"
     },
     "user_tz": 300
    },
    "id": "Oiy317nYtZDK"
   },
   "outputs": [],
   "source": [
    "class TrainConfig:\n",
    "\n",
    "    # Parameters to modify:\n",
    "    batch_size: int = 64  # How many batches per training step\n",
    "    max_iters: int = 2000  # Total of training iterations\n",
    "    learning_rate: float=1e-3 # Learning rate\n",
    "    grad_clip: float=1.0 # Maximium magnitude of gradient\n",
    "    eval_interval: int=50 # How often to evaluate the model\n",
    "    eval_iters: int=10 # Number of iterations to average for evaluation\n",
    "    seed: int=1337 # Random seed (can change the results)\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # These are responsible for correct training given GPU (DO NOT MODIFY)\n",
    "    dtype: str =  'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "    ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "    ctx = nullcontext() if device == 'cpu' else torch.amp.autocast(device_type=device, dtype=ptdtype)\n",
    "    scaler = torch.amp.GradScaler(device,enabled=(dtype == 'float16'))\n",
    "\n",
    "    # Populated by the script (DO NOT MODIFY)\n",
    "    train_dataloader: None\n",
    "    test_dataloader: None\n",
    "    optimizer: None\n",
    "\n",
    "class ModelConfig:\n",
    "    context_length: int = 256 # Number of tokens used for predicition\n",
    "    vocab_size: int = -1 # Number of words in the vocab (DO NOT MODIFY; changing the number here can make the model only recognize limited number of words!!!)\n",
    "    n_layer: int = 6 # Depth of the Transformer model (here: 6 Transformer Blocks)\n",
    "    n_head: int = 6 # Number of heads in the Multi-Head Attention\n",
    "    n_embd: int = 384 # Embedding dimension\n",
    "    dropout: float = 0.2 # Fraction used for drop-out; lower fraction -> more robust, but longer training (requires adjustment to the training time)\n",
    "    bias: bool = False # Whether or not to use a bias in the transformers layers\n",
    "    compile: bool = False # Whether to use the torch.compile (slows in the beginning of the training; faster training)\n",
    "    attn_dim: int = n_embd//n_head # Attention dimension (DO NOT MODIFY; changing the number here can break the model)\n",
    "\n",
    "\n",
    "model_config = ModelConfig()\n",
    "train_config = TrainConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1mmAPIwtZDL"
   },
   "source": [
    "Below, we define CUDA optimizations. This can controls whether TensorFloat-32 tensor cores may be used in matrix multiplications on Ampere or newer GPUs. It offers a significant speed-up, but might not be available on older GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1739738951368,
     "user": {
      "displayName": "Moin Mattar",
      "userId": "09338961536956502865"
     },
     "user_tz": 300
    },
    "id": "vvKgovAAtZDL"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(train_config.seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owpYPyIZtZDM"
   },
   "source": [
    "Data Loading function. Here, we get the necessary vocabulary for the training and perform a simple training/testing split. No need to change anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1739738951368,
     "user": {
      "displayName": "Moin Mattar",
      "userId": "09338961536956502865"
     },
     "user_tz": 300
    },
    "id": "NDs2w64ctZDM"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_dir = os.path.join('data', 'Shakespeare')\n",
    "model_config.vocab_size = getVocabSize(data_dir)\n",
    "train_config.train_dataloader, train_config.test_dataloader = getData(data_dir,model_config,train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qu7zLalstZDM"
   },
   "source": [
    "A simple definition of a feed-forward layer. No need to change anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1739738951369,
     "user": {
      "displayName": "Moin Mattar",
      "userId": "09338961536956502865"
     },
     "user_tz": 300
    },
    "id": "BU4FIHOotZDN"
   },
   "outputs": [],
   "source": [
    "# Define feed forward network\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, config.n_embd * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.n_embd * 4, config.n_embd),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ffn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1739738951369,
     "user": {
      "displayName": "Moin Mattar",
      "userId": "09338961536956502865"
     },
     "user_tz": 300
    },
    "id": "xKMkRSzGgrEf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from dataset import getData, getVocabSize\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "from utils import train, inference\n",
    "import math\n",
    "# Import torch.nn to access nn.Module, nn.Linear, etc.\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as function\n",
    "\n",
    "# ... (Rest of the code remains unchanged) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfQ4vzD8tZDN"
   },
   "source": [
    "### IMPLEMENTATION REQUIRED - Implement ``attention(self,q,k,v,T)`` of the Attention Module\n",
    "\n",
    "Below, we define the attention layer of the Transformer model. Here, you need to implement the attention mechanism. We define the attention as:\n",
    "$$ Attention(Q, K, V ) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V $$\n",
    "Nevertheless, the original attention can easily overfit to the data. To allivate that, we introduce an additional dropout layer. For your convenience, we split the implementation into two steps:\n",
    "$$weights = \\frac{QK^T}{\\sqrt{d_k}}$$\n",
    "$$attention = \\text{softmax}(\\text{dropout}(weights))V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1739738951369,
     "user": {
      "displayName": "Moin Mattar",
      "userId": "09338961536956502865"
     },
     "user_tz": 300
    },
    "id": "dlseuHwLtZDN"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        self.Wq = nn.Linear(config.n_embd, config.attn_dim, bias=config.bias)\n",
    "        self.Wk = nn.Linear(config.n_embd, config.attn_dim, bias=config.bias)\n",
    "        self.Wv = nn.Linear(config.n_embd, config.attn_dim, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.context_length, config.context_length, requires_grad=False)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q = self.Wq(x)\n",
    "        k = self.Wk(x)\n",
    "        v = self.Wv(x)\n",
    "        return self.attention(q,k,v,T)\n",
    "\n",
    "    def attention(self,q,k,v,T):\n",
    "      # Get dimension of key vectors\n",
    "      dk = k.size(-1)\n",
    "      \n",
    "      # Calculate attention weights using scaled dot product\n",
    "      weights = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(dk)\n",
    "      \n",
    "      # Apply causal mask to prevent attending to future tokens\n",
    "      weights = weights.masked_fill(self.mask[:T,:T] == 0, float('-inf'))\n",
    "      \n",
    "      # Apply softmax and dropout, then multiply with values\n",
    "      attention = torch.matmul(function.softmax(self.dropout(weights), dim=-1), v)\n",
    "\n",
    "      return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7STfhINMtZDO"
   },
   "source": [
    "### IMPLEMENTATION REQUIRED - Implement ``forward(self,x)`` of the MultiHeadAttention Module\n",
    "\n",
    "Below, we define the multi-head attention layer of the Transformer model. Here, you need to implement the multi-head attention mechanism defined as:\n",
    "$$MultiHead(x) = \\text{Dropout}(\\text{Concat}(\\text{head}_1, ..., \\text{head}_{\\text{heads}})W^O),$$\n",
    "$$ \\text{where head}_i = \\text{Attention}(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1739738951369,
     "user": {
      "displayName": "Moin Mattar",
      "userId": "09338961536956502865"
     },
     "user_tz": 300
    },
    "id": "6OTaOFVwtZDO"
   },
   "outputs": [],
   "source": [
    "# Define Multi-head Attention ｜\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.heads = nn.ModuleList([Attention(config) for _ in range(self.config.n_head)])\n",
    "        self.projection_layer = nn.Linear(self.config.n_embd, self.config.n_embd)\n",
    "        self.dropout = nn.Dropout(self.config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "      # Apply attention to each head in parallel\n",
    "      outputs = [attention_head(x) for attention_head in self.heads]\n",
    "      # Combine head outputs along embedding dimension\n",
    "      combined = torch.cat(outputs, dim=-1)\n",
    "      # Project concatenated heads\n",
    "      return self.dropout(self.projection_layer(combined))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7wvueCTtZDO"
   },
   "source": [
    "Finally, we are able to define the standard Transformer Block. No changes required here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1739738951369,
     "user": {
      "displayName": "Moin Mattar",
      "userId": "09338961536956502865"
     },
     "user_tz": 300
    },
    "id": "zrrhqKHktZDO"
   },
   "outputs": [],
   "source": [
    "# Define Transformer Block ｜\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mha = MultiHeadAttention(config)\n",
    "        self.ffn = FeedForwardNetwork(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EHL7MostZDP"
   },
   "source": [
    "### IMPLEMENTATION REQUIRED - Implement ``__init__`` of Positional Encoding\n",
    "\n",
    "Below, we define the Positional Encoding of the Transformer architecture. The positional encoding gives a specific value based on the token position in the input data. Therefore, a positional encoding can be seen as a feature defined only based on the position of each token. We can precompute it as:\n",
    "$$PE(pos,2i) = \\sin(\\text{pos}/div)$$\n",
    "$$PE(pos,2i+1) = \\cos(\\text{pos}/div),$$\n",
    "where $div=10000^{2i/dmodel}$ and the first equation defined the positional encoding for even tokens and the second one defines the encoding for the odd tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1739738951369,
     "user": {
      "displayName": "Moin Mattar",
      "userId": "09338961536956502865"
     },
     "user_tz": 300
    },
    "id": "u9mZx13WtZDP"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "  def __init__(self, config:ModelConfig):\n",
    "      super().__init__()\n",
    "      pos = torch.arange(0, config.context_length, requires_grad=False).unsqueeze(1)\n",
    "      div = torch.exp(torch.arange(0, config.n_embd, 2) * (math.log(10000.0) / config.n_embd))\n",
    "      pe = torch.zeros(config.context_length, config.n_embd, requires_grad=False)\n",
    "\n",
    "      pe[:, 0::2] = torch.sin(pos / div)\n",
    "      pe[:, 1::2] = torch.cos(pos / div)\n",
    "\n",
    "      self.register_buffer('pe', pe)\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    return self.pe[:x.size(1),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPhVE93DtZDP"
   },
   "source": [
    "Now, we define our model. We combine all our blocks into final Transfomer Model consisting of multiple Transformer blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739738951369,
     "user": {
      "displayName": "Moin Mattar",
      "userId": "09338961536956502865"
     },
     "user_tz": 300
    },
    "id": "mYPS1PRKtZDP"
   },
   "outputs": [],
   "source": [
    "# Define the model ｜\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, config:ModelConfig):\n",
    "        super().__init__()\n",
    "        self.tok_embedding = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.pos_embedding = PositionalEncoding(config)\n",
    "        self.transformer_blocks = nn.Sequential(*(\n",
    "                [TransformerBlock(config) for _ in range(config.n_layer)] +\n",
    "                [nn.LayerNorm(config.n_embd)]\n",
    "        ))\n",
    "        self.model_out_linear_layer = nn.Linear(config.n_embd, config.vocab_size)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        self.context_length = config.context_length\n",
    "\n",
    "    def forward(self, idx:torch.Tensor):\n",
    "        _, T = idx.shape\n",
    "        pos_emb = self.pos_embedding(idx)\n",
    "        tok_emb = self.tok_embedding(idx)\n",
    "\n",
    "        x = self.transformer_blocks(self.drop(tok_emb+pos_emb))\n",
    "        logits = self.model_out_linear_layer(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrnYfi6AtZDP"
   },
   "source": [
    "Now, we can initialize the model and, optionally, compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1739738951663,
     "user": {
      "displayName": "Moin Mattar",
      "userId": "09338961536956502865"
     },
     "user_tz": 300
    },
    "id": "Z_BfPL6PtZDQ"
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = Model(model_config).to(train_config.device)\n",
    "if model_config.compile:\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1E33C6JtZDQ"
   },
   "source": [
    "Finally, we can start the optimization process and start our training! This will take a bit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77,
     "referenced_widgets": [
      "706b7762048d44cb9a688d2102bccd65",
      "0ad58a78a7544611acad54da600c6b54",
      "b5cee4e2821c40b0bc4df1708e2909e3",
      "cbda6ba71eb941b4ace8e878fc587325",
      "445c819ee30d40e89ca7da106d4dcf3b",
      "d97a72869bb74ddabee923e95ac06c3d",
      "d9f5e92ea59a4c6fb84715a6501c447f",
      "21f8a99fdc614211a9dcdc2cb98c117b",
      "cdc7a982433a446a9f4da9680b3d96f8",
      "5572d95edef1466484122ab8159d927c",
      "564c67a15703407da7b6834693b31337"
     ]
    },
    "id": "VeHq3EZGtZDQ",
    "outputId": "8402ae9e-c8fe-42b2-fb6d-c35c8dd404a3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706b7762048d44cb9a688d2102bccd65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the optimizer and train; Losses updated every eval_interval steps\n",
    "train_config.optimizer = torch.optim.AdamW(model.parameters(), lr=train_config.learning_rate)\n",
    "train(model,train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqb7N_R-tZDQ"
   },
   "source": [
    "Here, you can save the model for further use. We will use this to show you how to load a model in other applications below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsbOzo0ctZDQ"
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model/model.ckpt')\n",
    "with open('model/model_config.pkl','wb') as f:\n",
    "    pickle.dump(model_config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6ALuScwtZDQ"
   },
   "source": [
    "Configuration used for inference. Feel free to modify it to your liking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9aoxRWttZDR"
   },
   "outputs": [],
   "source": [
    "class InferenceConfig():\n",
    "    seed:int=0 # Random seed (impacts the output)\n",
    "    start:str=\"ROMEO:\" # Starting prompt to generate from\n",
    "    temperature:float = 0.7 # Degree of 'creativity': 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "    max_new_tokens:int=250 # Length of the generated sequence in tokens\n",
    "    top_k:int=None  # Retain only the top k most likely tokens, clamp others to have 0 probability (None - no clamp)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OuMzdHfqtZDR"
   },
   "source": [
    "As previously, we define our CUDA operations if possible. Use the same CUDA config as the one above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C71dlYXvtZDR"
   },
   "outputs": [],
   "source": [
    "inference_config = InferenceConfig()\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "torch.manual_seed(inference_config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDu2QhudtZDR"
   },
   "source": [
    "Here we load the model and optionally compile it. As the `meta_path`, we load the information about the vocabulary we trained the model on to help it with generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GpoyZzm-tZDR"
   },
   "outputs": [],
   "source": [
    "# Load the model and hyperparameters ｜\n",
    "with open('model/model_config.pkl', 'rb') as f:\n",
    "    model_config = pickle.load(f)\n",
    "\n",
    "model = Model(model_config)\n",
    "if model_config.compile:\n",
    "    model = torch.compile(model)\n",
    "model.load_state_dict(torch.load('model/model.ckpt', weights_only=True),strict=False)\n",
    "model.eval()\n",
    "model.to(inference_config.device)\n",
    "\n",
    "inference_config.meta_path = os.path.join('data', 'Shakespeare', 'meta.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0B_lIF6otZDR"
   },
   "source": [
    "Now, you can generate your text here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*VIRGILIA:\n",
      "'T*is not to save l*abour, nor that I want lo&v%e.\n",
      "\n",
      "VALERIA:\n",
      "Y#ou would be anothe#r P#enelope: yet, the$y say, all\n",
      "%the yarn she s%pun in Ulysses' absence did but fill\n",
      "Ithaca* full of moths. Come;% I would@ $your cambr%ic\n",
      "were% sensible as *your finger, tha%t you !might @lea!ve@\n",
      "pricking& i&t@ for pity#. Co!me, *you shall go with us.\n"
     ]
    }
   ],
   "source": [
    "# Generate text\n",
    "print(inference(model, inference_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYEgZviNtZDS"
   },
   "source": [
    "To see how big the model is, you can run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YhDikn6htZDS"
   },
   "outputs": [],
   "source": [
    "# Optionally, print model total of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ad58a78a7544611acad54da600c6b54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d97a72869bb74ddabee923e95ac06c3d",
      "placeholder": "​",
      "style": "IPY_MODEL_d9f5e92ea59a4c6fb84715a6501c447f",
      "value": "  0%"
     }
    },
    "21f8a99fdc614211a9dcdc2cb98c117b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "445c819ee30d40e89ca7da106d4dcf3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5572d95edef1466484122ab8159d927c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "564c67a15703407da7b6834693b31337": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "706b7762048d44cb9a688d2102bccd65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0ad58a78a7544611acad54da600c6b54",
       "IPY_MODEL_b5cee4e2821c40b0bc4df1708e2909e3",
       "IPY_MODEL_cbda6ba71eb941b4ace8e878fc587325"
      ],
      "layout": "IPY_MODEL_445c819ee30d40e89ca7da106d4dcf3b"
     }
    },
    "b5cee4e2821c40b0bc4df1708e2909e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_21f8a99fdc614211a9dcdc2cb98c117b",
      "max": 2000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cdc7a982433a446a9f4da9680b3d96f8",
      "value": 0
     }
    },
    "cbda6ba71eb941b4ace8e878fc587325": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5572d95edef1466484122ab8159d927c",
      "placeholder": "​",
      "style": "IPY_MODEL_564c67a15703407da7b6834693b31337",
      "value": " 0/2000 [00:00&lt;?, ?it/s, Training Loss: -1 Validation Loss: -1]"
     }
    },
    "cdc7a982433a446a9f4da9680b3d96f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d97a72869bb74ddabee923e95ac06c3d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9f5e92ea59a4c6fb84715a6501c447f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
